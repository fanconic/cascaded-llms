seed: 42
results_dir: "results"
max_new_tokens: 128
base_gen_cost: 1.0
large_inf_cost: 1.0
large_gen_cost: 25.0
expert_cost: 100.0
base_model: "HuggingFaceTB/SmolLM-1.7B-Instruct"
large_model: "meta-llama/Llama-3.1-8B-Instruct"
probability_threshold: 1.0
uncertainty_threshold: 1.0
batch_size: 7
max_input_length: 512
num_samples: 21
verification_fn: "surrogate_token_probs"
uncertainty_fn: "per_token_entropy"